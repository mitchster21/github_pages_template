[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n[Your Degree] - [University Name], [Year]\nRelevant Coursework: Statistics, Data Analysis, Programming, etc."
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\n[Your specific interests, e.g., environmental data, healthcare analytics, finance, etc.]\n[Other areas you’re curious about]"
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nDescribe what you hope to achieve through your data science journey:\n\nShort-term learning objectives\nLong-term career aspirations\nTypes of problems you want to solve"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: your.email@example.com\nGitHub: github.com/your-username\nLinkedIn: linkedin.com/in/your-profile\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills."
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills."
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later."
  },
  {
    "objectID": "mitchster21.github.io/index.html",
    "href": "mitchster21.github.io/index.html",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Goal: Compare a simple, interpretable model (logistic regression) with a more flexible, complex model (random forest) for predicting whether the home team wins. We’ll walk through data simulation, training, evaluation, and interpretation.\n\n\n\nTo keep things simple, we simulate a small sports dataset in a separate Python script compare_models.py. Each row represents a matchup with:\n\nhome_score: points scored by the home team\n\naway_score: points scored by the away team\n\nhome_advantage: a feature that encodes “home-court” advantage\n\ntarget: whether the home team won (1) or lost (0)\n\nThis practice data keeps the focus on model comparison instead of data wrangling.\n\n\n\n\nTo demonstrate how logistic regression and random forest handle different data patterns, we simulate a small sports dataset. Outcomes depend on nonlinear effects and interactions, which are easier for random forests to capture than logistic regression.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Simulate a simple matchup dataset\nn_games = 2000\nhome = np.random.binomial(1, 0.5, n_games)\nstrength_diff = np.random.normal(0, 1, n_games)\nrest_diff = np.random.normal(0, 1, n_games)\n\n# Outcome depends on nonlinear interactions\nlin = 1.2 * home * strength_diff - 0.7 * rest_diff**2 + 0.5 * strength_diff**3\np_home_win = 1 / (1 + np.exp(-lin))\ny = np.random.binomial(1, p_home_win)\n\ndf = pd.DataFrame({\n    \"home\": home,\n    \"strength_diff\": strength_diff,\n    \"rest_diff\": rest_diff,\n    \"home_win\": y\n})\n\ndf.head()\n\n\n\n\n\n\n\n\n\nhome\nstrength_diff\nrest_diff\nhome_win\n\n\n\n\n0\n1\n0.233422\n-0.842243\n1\n\n\n1\n0\n0.076516\n0.015178\n0\n\n\n2\n0\n-0.588007\n0.090089\n0\n\n\n3\n0\n0.912361\n-0.790684\n1\n\n\n4\n0\n-0.941359\n0.308567\n0\n\n\n\n\n\n\n\n\n\n\nLogistic regression is the baseline. It assumes a linear relationship between features and the log-odds of winning.\n\nPros: fast, interpretable, coefficients tell us feature impact\n\nCons: can miss nonlinear interactions\n\nHere’s the confusion matrix for Logistic Regression:\n\n\n\n\n\nRandom forest is an ensemble of decision trees. It captures nonlinear patterns and interactions.\n\nPros: higher flexibility, often better accuracy\n\nCons: less interpretable, requires more tuning\n\nHere’s the confusion matrix for Random Forest:\n\n\n\n\n\n\nWe train both logistic regression (LR) and random forest (RF) on the simulated dataset. Logistic regression assumes a linear relationship, while random forest can capture nonlinear effects and interactions.\n\n\n\nCode\nimport json\nimport pandas as pd\nimport os\n\n# Load metrics\nwith open(\"results/metrics.json\") as f:\n    results = json.load(f)\n\ndf = pd.DataFrame([\n    {\"Model\": \"Logistic Regression\", \"Accuracy\": results[\"Logistic Regression\"], \"Notes\": \"Simple, interpretable\"},\n    {\"Model\": \"Random Forest\", \"Accuracy\": results[\"Random Forest\"], \"Notes\": \"Captures nonlinear patterns\"}\n])\n\ndf\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nNotes\n\n\n\n\n0\nLogistic Regression\n0.750000\nSimple, interpretable\n\n\n1\nRandom Forest\n0.771667\nCaptures nonlinear patterns\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression is simple, interpretable, and explains feature impact clearly. It performs well when relationships are mostly linear.\n\nRandom Forest captures nonlinear patterns and feature interactions, giving it a slight edge in predictive accuracy for this dataset.\n\nTakeaway: In sports analytics (and most applied work), there’s a tradeoff: choose logistic regression when you need transparency, and random forest when you want the best predictive performance.\nCall to Action: Try running the same comparison on your own dataset from your favorite sports team. Download their stats, simulate or process relevant features, and test linear vs nonlinear models. See for yourself how model choice can influence your predictions."
  },
  {
    "objectID": "mitchster21.github.io/index.html#introduction-the-dataset",
    "href": "mitchster21.github.io/index.html#introduction-the-dataset",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "To keep things simple, we simulate a small sports dataset in a separate Python script compare_models.py. Each row represents a matchup with:\n\nhome_score: points scored by the home team\n\naway_score: points scored by the away team\n\nhome_advantage: a feature that encodes “home-court” advantage\n\ntarget: whether the home team won (1) or lost (0)\n\nThis practice data keeps the focus on model comparison instead of data wrangling."
  },
  {
    "objectID": "mitchster21.github.io/index.html#simulating-game-outcomes",
    "href": "mitchster21.github.io/index.html#simulating-game-outcomes",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "To demonstrate how logistic regression and random forest handle different data patterns, we simulate a small sports dataset. Outcomes depend on nonlinear effects and interactions, which are easier for random forests to capture than logistic regression.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Simulate a simple matchup dataset\nn_games = 2000\nhome = np.random.binomial(1, 0.5, n_games)\nstrength_diff = np.random.normal(0, 1, n_games)\nrest_diff = np.random.normal(0, 1, n_games)\n\n# Outcome depends on nonlinear interactions\nlin = 1.2 * home * strength_diff - 0.7 * rest_diff**2 + 0.5 * strength_diff**3\np_home_win = 1 / (1 + np.exp(-lin))\ny = np.random.binomial(1, p_home_win)\n\ndf = pd.DataFrame({\n    \"home\": home,\n    \"strength_diff\": strength_diff,\n    \"rest_diff\": rest_diff,\n    \"home_win\": y\n})\n\ndf.head()\n\n\n\n\n\n\n\n\n\nhome\nstrength_diff\nrest_diff\nhome_win\n\n\n\n\n0\n1\n0.233422\n-0.842243\n1\n\n\n1\n0\n0.076516\n0.015178\n0\n\n\n2\n0\n-0.588007\n0.090089\n0\n\n\n3\n0\n0.912361\n-0.790684\n1\n\n\n4\n0\n-0.941359\n0.308567\n0"
  },
  {
    "objectID": "mitchster21.github.io/index.html#simple-model-logistic-regression",
    "href": "mitchster21.github.io/index.html#simple-model-logistic-regression",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Logistic regression is the baseline. It assumes a linear relationship between features and the log-odds of winning.\n\nPros: fast, interpretable, coefficients tell us feature impact\n\nCons: can miss nonlinear interactions\n\nHere’s the confusion matrix for Logistic Regression:"
  },
  {
    "objectID": "mitchster21.github.io/index.html#complex-model-random-forest",
    "href": "mitchster21.github.io/index.html#complex-model-random-forest",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Random forest is an ensemble of decision trees. It captures nonlinear patterns and interactions.\n\nPros: higher flexibility, often better accuracy\n\nCons: less interpretable, requires more tuning\n\nHere’s the confusion matrix for Random Forest:"
  },
  {
    "objectID": "mitchster21.github.io/index.html#compare-results",
    "href": "mitchster21.github.io/index.html#compare-results",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "We train both logistic regression (LR) and random forest (RF) on the simulated dataset. Logistic regression assumes a linear relationship, while random forest can capture nonlinear effects and interactions.\n\n\n\nCode\nimport json\nimport pandas as pd\nimport os\n\n# Load metrics\nwith open(\"results/metrics.json\") as f:\n    results = json.load(f)\n\ndf = pd.DataFrame([\n    {\"Model\": \"Logistic Regression\", \"Accuracy\": results[\"Logistic Regression\"], \"Notes\": \"Simple, interpretable\"},\n    {\"Model\": \"Random Forest\", \"Accuracy\": results[\"Random Forest\"], \"Notes\": \"Captures nonlinear patterns\"}\n])\n\ndf\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nNotes\n\n\n\n\n0\nLogistic Regression\n0.750000\nSimple, interpretable\n\n\n1\nRandom Forest\n0.771667\nCaptures nonlinear patterns"
  },
  {
    "objectID": "mitchster21.github.io/index.html#conclusion",
    "href": "mitchster21.github.io/index.html#conclusion",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Logistic Regression is simple, interpretable, and explains feature impact clearly. It performs well when relationships are mostly linear.\n\nRandom Forest captures nonlinear patterns and feature interactions, giving it a slight edge in predictive accuracy for this dataset.\n\nTakeaway: In sports analytics (and most applied work), there’s a tradeoff: choose logistic regression when you need transparency, and random forest when you want the best predictive performance.\nCall to Action: Try running the same comparison on your own dataset from your favorite sports team. Download their stats, simulate or process relevant features, and test linear vs nonlinear models. See for yourself how model choice can influence your predictions."
  },
  {
    "objectID": "mitchster21.github.io/about.html",
    "href": "mitchster21.github.io/about.html",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "mitchster21.github.io/about.html#background",
    "href": "mitchster21.github.io/about.html#background",
    "title": "About Me",
    "section": "",
    "text": "Write a brief introduction about yourself here. Include:\n\nYour academic background\nYour interest in data science\nYour career goals\nAny relevant experience or projects"
  },
  {
    "objectID": "mitchster21.github.io/about.html#education",
    "href": "mitchster21.github.io/about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\n[Your Degree] - [University Name], [Year]\nRelevant Coursework: Statistics, Data Analysis, Programming, etc."
  },
  {
    "objectID": "mitchster21.github.io/about.html#skills-interests",
    "href": "mitchster21.github.io/about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nTechnical Skills\n\nProgramming: Python\nData Analysis: Pandas, NumPy\nVisualization: Matplotlib, Seaborn\nMachine Learning: Scikit-learn\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\n\n[Your specific interests, e.g., environmental data, healthcare analytics, finance, etc.]\n[Other areas you’re curious about]"
  },
  {
    "objectID": "mitchster21.github.io/about.html#goals",
    "href": "mitchster21.github.io/about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nDescribe what you hope to achieve through your data science journey:\n\nShort-term learning objectives\nLong-term career aspirations\nTypes of problems you want to solve"
  },
  {
    "objectID": "mitchster21.github.io/about.html#contact",
    "href": "mitchster21.github.io/about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: your.email@example.com\nGitHub: github.com/your-username\nLinkedIn: linkedin.com/in/your-profile\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Goal: Compare a simple, interpretable model (logistic regression) with a more flexible, complex model (random forest) for predicting whether the home team wins. We’ll walk through data simulation, training, evaluation, and interpretation.\n\n\n\nTo keep things simple, we simulate a small sports dataset in a separate Python script compare_models.py. Each row represents a matchup with:\n\nhome_score: points scored by the home team\n\naway_score: points scored by the away team\n\nhome_advantage: a feature that encodes “home-court” advantage\n\ntarget: whether the home team won (1) or lost (0)\n\nThis practice data keeps the focus on model comparison instead of data wrangling.\n\n\n\n\nTo demonstrate how logistic regression and random forest handle different data patterns, we simulate a small sports dataset. Outcomes depend on nonlinear effects and interactions, which are easier for random forests to capture than logistic regression.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Simulate a simple matchup dataset\nn_games = 2000\nhome = np.random.binomial(1, 0.5, n_games)\nstrength_diff = np.random.normal(0, 1, n_games)\nrest_diff = np.random.normal(0, 1, n_games)\n\n# Outcome depends on nonlinear interactions\nlin = 1.2 * home * strength_diff - 0.7 * rest_diff**2 + 0.5 * strength_diff**3\np_home_win = 1 / (1 + np.exp(-lin))\ny = np.random.binomial(1, p_home_win)\n\ndf = pd.DataFrame({\n    \"home\": home,\n    \"strength_diff\": strength_diff,\n    \"rest_diff\": rest_diff,\n    \"home_win\": y\n})\n\ndf.head()\n\n\n\n\n\n\n\n\n\nhome\nstrength_diff\nrest_diff\nhome_win\n\n\n\n\n0\n1\n0.853197\n1.119395\n0\n\n\n1\n1\n1.074322\n-0.087479\n1\n\n\n2\n1\n-0.614455\n0.111486\n0\n\n\n3\n1\n-0.022522\n-2.586458\n0\n\n\n4\n1\n-0.000453\n-0.545446\n1\n\n\n\n\n\n\n\n\n\n\nLogistic regression is the baseline. It assumes a linear relationship between features and the log-odds of winning.\n\nPros: fast, interpretable, coefficients tell us feature impact\n\nCons: can miss nonlinear interactions\n\nHere’s the confusion matrix for Logistic Regression:\n\n\n\n\n\nRandom forest is an ensemble of decision trees. It captures nonlinear patterns and interactions.\n\nPros: higher flexibility, often better accuracy\n\nCons: less interpretable, requires more tuning\n\nHere’s the confusion matrix for Random Forest:\n\n\n\n\n\n\nWe train both logistic regression (LR) and random forest (RF) on the simulated dataset. Logistic regression assumes a linear relationship, while random forest can capture nonlinear effects and interactions.\n\n\n\nCode\nimport json\nimport pandas as pd\nimport os\n\n# Load metrics\nwith open(\"results/metrics.json\") as f:\n    results = json.load(f)\n\ndf = pd.DataFrame([\n    {\"Model\": \"Logistic Regression\", \"Accuracy\": results[\"Logistic Regression\"], \"Notes\": \"Simple, interpretable\"},\n    {\"Model\": \"Random Forest\", \"Accuracy\": results[\"Random Forest\"], \"Notes\": \"Captures nonlinear patterns\"}\n])\n\ndf\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nNotes\n\n\n\n\n0\nLogistic Regression\n0.750000\nSimple, interpretable\n\n\n1\nRandom Forest\n0.771667\nCaptures nonlinear patterns\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression is simple, interpretable, and explains feature impact clearly. It performs well when relationships are mostly linear.\n\nRandom Forest captures nonlinear patterns and feature interactions, giving it a slight edge in predictive accuracy for this dataset.\n\nTakeaway: In sports analytics (and most applied work), there’s a tradeoff: choose logistic regression when you need transparency, and random forest when you want the best predictive performance.\nCall to Action: Try running the same comparison on your own dataset from your favorite sports team. Download their stats, simulate or process relevant features, and test linear vs nonlinear models. See for yourself how model choice can influence your predictions.",
    "crumbs": []
  },
  {
    "objectID": "index.html#introduction-the-dataset",
    "href": "index.html#introduction-the-dataset",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "To keep things simple, we simulate a small sports dataset in a separate Python script compare_models.py. Each row represents a matchup with:\n\nhome_score: points scored by the home team\n\naway_score: points scored by the away team\n\nhome_advantage: a feature that encodes “home-court” advantage\n\ntarget: whether the home team won (1) or lost (0)\n\nThis practice data keeps the focus on model comparison instead of data wrangling.",
    "crumbs": []
  },
  {
    "objectID": "index.html#simulating-game-outcomes",
    "href": "index.html#simulating-game-outcomes",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "To demonstrate how logistic regression and random forest handle different data patterns, we simulate a small sports dataset. Outcomes depend on nonlinear effects and interactions, which are easier for random forests to capture than logistic regression.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# Simulate a simple matchup dataset\nn_games = 2000\nhome = np.random.binomial(1, 0.5, n_games)\nstrength_diff = np.random.normal(0, 1, n_games)\nrest_diff = np.random.normal(0, 1, n_games)\n\n# Outcome depends on nonlinear interactions\nlin = 1.2 * home * strength_diff - 0.7 * rest_diff**2 + 0.5 * strength_diff**3\np_home_win = 1 / (1 + np.exp(-lin))\ny = np.random.binomial(1, p_home_win)\n\ndf = pd.DataFrame({\n    \"home\": home,\n    \"strength_diff\": strength_diff,\n    \"rest_diff\": rest_diff,\n    \"home_win\": y\n})\n\ndf.head()\n\n\n\n\n\n\n\n\n\nhome\nstrength_diff\nrest_diff\nhome_win\n\n\n\n\n0\n1\n0.853197\n1.119395\n0\n\n\n1\n1\n1.074322\n-0.087479\n1\n\n\n2\n1\n-0.614455\n0.111486\n0\n\n\n3\n1\n-0.022522\n-2.586458\n0\n\n\n4\n1\n-0.000453\n-0.545446\n1",
    "crumbs": []
  },
  {
    "objectID": "index.html#simple-model-logistic-regression",
    "href": "index.html#simple-model-logistic-regression",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Logistic regression is the baseline. It assumes a linear relationship between features and the log-odds of winning.\n\nPros: fast, interpretable, coefficients tell us feature impact\n\nCons: can miss nonlinear interactions\n\nHere’s the confusion matrix for Logistic Regression:",
    "crumbs": []
  },
  {
    "objectID": "index.html#complex-model-random-forest",
    "href": "index.html#complex-model-random-forest",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Random forest is an ensemble of decision trees. It captures nonlinear patterns and interactions.\n\nPros: higher flexibility, often better accuracy\n\nCons: less interpretable, requires more tuning\n\nHere’s the confusion matrix for Random Forest:",
    "crumbs": []
  },
  {
    "objectID": "index.html#compare-results",
    "href": "index.html#compare-results",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "We train both logistic regression (LR) and random forest (RF) on the simulated dataset. Logistic regression assumes a linear relationship, while random forest can capture nonlinear effects and interactions.\n\n\n\nCode\nimport json\nimport pandas as pd\nimport os\n\n# Load metrics\nwith open(\"results/metrics.json\") as f:\n    results = json.load(f)\n\ndf = pd.DataFrame([\n    {\"Model\": \"Logistic Regression\", \"Accuracy\": results[\"Logistic Regression\"], \"Notes\": \"Simple, interpretable\"},\n    {\"Model\": \"Random Forest\", \"Accuracy\": results[\"Random Forest\"], \"Notes\": \"Captures nonlinear patterns\"}\n])\n\ndf\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nNotes\n\n\n\n\n0\nLogistic Regression\n0.750000\nSimple, interpretable\n\n\n1\nRandom Forest\n0.771667\nCaptures nonlinear patterns",
    "crumbs": []
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Random Forest vs Logistic Regression: Predicting Game Outcomes",
    "section": "",
    "text": "Logistic Regression is simple, interpretable, and explains feature impact clearly. It performs well when relationships are mostly linear.\n\nRandom Forest captures nonlinear patterns and feature interactions, giving it a slight edge in predictive accuracy for this dataset.\n\nTakeaway: In sports analytics (and most applied work), there’s a tradeoff: choose logistic regression when you need transparency, and random forest when you want the best predictive performance.\nCall to Action: Try running the same comparison on your own dataset from your favorite sports team. Download their stats, simulate or process relevant features, and test linear vs nonlinear models. See for yourself how model choice can influence your predictions.",
    "crumbs": []
  }
]